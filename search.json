[
  {
    "objectID": "files/index-spa.html",
    "href": "files/index-spa.html",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "",
    "text": "El énfasis de este taller está en evaluar la estructura y comparar pruebas educativas y psicológicas en grupos independientes de encuestados. A menudo, queremos utilizar datos de pruebas para comparar grupos usando el constructo que se está midiendo, por ejemplo, al comparar las medias del grupo en la puntuación total de la prueba. Sin embargo, las puntuaciones de las pruebas también pueden estar influenciadas por atributos del encuestado que no sean el rasgo que se está midiendo. En los Estados Unidos, un ejemplo común son las evaluaciones de la depresión que preguntan si una persona ha llorado en la última semana. Los hombres tienen menos probabilidad de respaldar este ítem que las mujeres, incluso cuando tienen el mismo nivel de depresión (Bauer, 2017). Por lo tanto, comparar hombres y mujeres en términos de su puntuación total en la prueba llevaría a subestimar el nivel de depresión en los hombres.\nEste taller introduce nuevas herramientas de código abierto que prometen facilitar el análisis y la desagregación de datos de pruebas educativas y psicológicas, garantizando la equidad de los datos. Cubriendo el análisis factorial k-fold (kfa), la invarianza de medición (IM) y el funcionamiento diferencial de ítems (FDI), el taller se organiza de la siguiente manera:\n\nLa parte 1 aborda cómo establecer la estructura de la evaluación: ¿Cuántas dimensiones tiene la evaluación? ¿Qué ítems corresponden a qué dimensiones? En este entrenamiento, presentaremos un nuevo recurso de código abierto para identificar y confirmar eficientemente la estructura de una evaluación: el paquete R de validación cruzada k-fold para análisis factorial (kfa). Kfa combina un enfoque analítico factorial basado en modelos exploratorios y confirmatorios que aprovecha eficientemente todos los datos disponibles y proporciona información sintetizada fácil de usar sobre la estructura de una evaluación, ayudando a lograr un consenso en torno al desarrollo y puntuación de escalas.\nLa parte 2 aborda la Invarianza de Medición (MI) en el modelo de factor unidimensional para datos categóricos. El enfoque se centra en varios “niveles” de MI que se obtienen al imponer restricciones cada vez más estrictas en el modelo (por ejemplo, configuracional, débil/métrica, fuerte/escalar) y cómo estas restricciones pueden ser probadas utilizando datos empíricos.\nLa parte 3 aborda qué hacer cuando se rechaza la Invarianza de Medición (MI, por sus siglas en inglés). En este caso, se puede concluir que uno o más elementos de evaluación muestran DIF, también conocido como MI parcial. Los métodos para inferir qué elementos muestran DIF fueron desarrollados originalmente en el contexto de la teoría de respuesta al ítem (IRT, por sus siglas en inglés), en lugar del análisis factorial. Esta parte del taller comienza con una revisión de la IRT, enfatizando su relación con el análisis factorial para datos categóricos, y luego se discuten algunas técnicas tradicionales de análisis de DIF (la prueba de Mantel-Haenszel y la de razón de verosimilitud). Se presta especial atención al problema de la selección de ítems de anclaje.\nLa parte 4 incluye una revisión de algunos enfoques más recientes para el análisis de DIF, centrándose en el trabajo del autor sobre cómo abordar el DIF como un problema para hacer un escalamiento robusto (Halpin, 2022) y extensiones de este enfoque para evaluar si el DIF afecta las conclusiones sobre cómo difieren los grupos en el constructo medido (DTF)."
  },
  {
    "objectID": "files/index-spa.html#descripción-general",
    "href": "files/index-spa.html#descripción-general",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "",
    "text": "El énfasis de este taller está en evaluar la estructura y comparar pruebas educativas y psicológicas en grupos independientes de encuestados. A menudo, queremos utilizar datos de pruebas para comparar grupos usando el constructo que se está midiendo, por ejemplo, al comparar las medias del grupo en la puntuación total de la prueba. Sin embargo, las puntuaciones de las pruebas también pueden estar influenciadas por atributos del encuestado que no sean el rasgo que se está midiendo. En los Estados Unidos, un ejemplo común son las evaluaciones de la depresión que preguntan si una persona ha llorado en la última semana. Los hombres tienen menos probabilidad de respaldar este ítem que las mujeres, incluso cuando tienen el mismo nivel de depresión (Bauer, 2017). Por lo tanto, comparar hombres y mujeres en términos de su puntuación total en la prueba llevaría a subestimar el nivel de depresión en los hombres.\nEste taller introduce nuevas herramientas de código abierto que prometen facilitar el análisis y la desagregación de datos de pruebas educativas y psicológicas, garantizando la equidad de los datos. Cubriendo el análisis factorial k-fold (kfa), la invarianza de medición (IM) y el funcionamiento diferencial de ítems (FDI), el taller se organiza de la siguiente manera:\n\nLa parte 1 aborda cómo establecer la estructura de la evaluación: ¿Cuántas dimensiones tiene la evaluación? ¿Qué ítems corresponden a qué dimensiones? En este entrenamiento, presentaremos un nuevo recurso de código abierto para identificar y confirmar eficientemente la estructura de una evaluación: el paquete R de validación cruzada k-fold para análisis factorial (kfa). Kfa combina un enfoque analítico factorial basado en modelos exploratorios y confirmatorios que aprovecha eficientemente todos los datos disponibles y proporciona información sintetizada fácil de usar sobre la estructura de una evaluación, ayudando a lograr un consenso en torno al desarrollo y puntuación de escalas.\nLa parte 2 aborda la Invarianza de Medición (MI) en el modelo de factor unidimensional para datos categóricos. El enfoque se centra en varios “niveles” de MI que se obtienen al imponer restricciones cada vez más estrictas en el modelo (por ejemplo, configuracional, débil/métrica, fuerte/escalar) y cómo estas restricciones pueden ser probadas utilizando datos empíricos.\nLa parte 3 aborda qué hacer cuando se rechaza la Invarianza de Medición (MI, por sus siglas en inglés). En este caso, se puede concluir que uno o más elementos de evaluación muestran DIF, también conocido como MI parcial. Los métodos para inferir qué elementos muestran DIF fueron desarrollados originalmente en el contexto de la teoría de respuesta al ítem (IRT, por sus siglas en inglés), en lugar del análisis factorial. Esta parte del taller comienza con una revisión de la IRT, enfatizando su relación con el análisis factorial para datos categóricos, y luego se discuten algunas técnicas tradicionales de análisis de DIF (la prueba de Mantel-Haenszel y la de razón de verosimilitud). Se presta especial atención al problema de la selección de ítems de anclaje.\nLa parte 4 incluye una revisión de algunos enfoques más recientes para el análisis de DIF, centrándose en el trabajo del autor sobre cómo abordar el DIF como un problema para hacer un escalamiento robusto (Halpin, 2022) y extensiones de este enfoque para evaluar si el DIF afecta las conclusiones sobre cómo difieren los grupos en el constructo medido (DTF)."
  },
  {
    "objectID": "files/index-spa.html#requisitos",
    "href": "files/index-spa.html#requisitos",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "Requisitos",
    "text": "Requisitos\nEl curso presupone conocimientos sobre análisis factorial y/o teoría de respuesta al ítem. Las ilustraciones numéricas utilizan el lenguaje de programación R. Para asegurarse de poder seguir los ejemplos numéricos, se recomienda instalar las siguientes bibliotecas antes de asistir al taller.\n\n# Data manipulation and summary statistics\ninstall.packages(\"psych\")\ninstall.packages(\"gtsummary\")\ninstall.packages(\"dplyr\")\n\n# Factor analysis\ninstall.packages (\"kfa\")\ninstall.packages(\"lavaan\")\n\n# IRT\ninstall.packages(\"mirt\")\ninstall.packages(\"difR\")\n\n# Robust DIF\ninstall.packages(\"remotes\")\nremotes::install_github(\"peterhalpin/robustDIF\")\n\n# Plotting\ninstall.packages(\"ggplot2\")"
  },
  {
    "objectID": "files/index-spa.html#datos",
    "href": "files/index-spa.html#datos",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "Datos",
    "text": "Datos\nEl conjunto de datos a continuación se utiliza para hacer ilustraciones numéricas. Tenga en cuenta que el acceso a los datos está protegido por una contraseña. La contraseña se compartirá con los participantes en la primera sesión del taller.\n\nLink to data"
  },
  {
    "objectID": "files/index-spa.html#agenda-y-materiales",
    "href": "files/index-spa.html#agenda-y-materiales",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "Agenda y materiales",
    "text": "Agenda y materiales\n\n\n\nSesion\nMateriales\nSuplementos\n\n\n\n\nPre-workshop R prep\nCode\n\n\n\nDay 1: KFA\n[Slides] [Code: Descriptive] [Code: kfa]\n\n\n\nDay 2, Part 1: MI\nSlides\nNotes\nCode\n\n\nReview of MI and DIF: Thissen, 2023\nMI for categorical data: Muthen & Asparouhov, 2002\nMillsap & Yun-Tein, 2004; Wu & Estabrook, 2016\n\n\n\nDay 2, Part 2: DIF\nSlides\nNotes\nCode\n\n\nFactor analysis and IRT: Wirth & Edwards, 2007\nReview of DIF: Teresi et. al, 2021\n\n\n\nDay 3, Part 3: RDIF\nSlides\nNotes\nCode\n\n\nRobust scaling: He & Cui, 2020\nRobust DIF: Halpin, 2022; Wang et al.,2022"
  },
  {
    "objectID": "files/index-eng.html",
    "href": "files/index-eng.html",
    "title": "MI / DIF Workshop",
    "section": "",
    "text": "This workshop on measurement invariance (MI) and differential item functioning (DIF) is organized as follows:\n\nPart 1 addresses MI in the unidimensional factor model for categorical data. The focus is on various “levels” of MI that are obtained by imposing increasingly strict constraints on the model (e.g., configural, weak / metric, strong / scalar), and how these constraints can be tested using empirical data.\nPart 2 addresses what to do when MI is rejected. In this case, it can be concluded that one or more assessment items exhibit DIF, which is also referred to as partial MI. Methods for inferring which items exhibit DIF were originally developed in the context of item response theory (IRT) rather than factor analysis. This part of the workshop begins with a review of IRT, emphasizing its relationship with factor analysis for categorical data, and then discusses some traditional techniques for DIF analysis (the Mantel-Haenszel test and the likelihood ratio test). Emphasis is given to the problem of anchor item selection.\nPart 3 reviews of some more recent approaches to DIF analysis, focusing on the author’s work on how DIF can be approached as a problem in robust scaling (Halpin, 2022) and extensions of this approach to evaluate differential test functioning (DTF)."
  },
  {
    "objectID": "files/index-eng.html#overview",
    "href": "files/index-eng.html#overview",
    "title": "MI / DIF Workshop",
    "section": "",
    "text": "This workshop on measurement invariance (MI) and differential item functioning (DIF) is organized as follows:\n\nPart 1 addresses MI in the unidimensional factor model for categorical data. The focus is on various “levels” of MI that are obtained by imposing increasingly strict constraints on the model (e.g., configural, weak / metric, strong / scalar), and how these constraints can be tested using empirical data.\nPart 2 addresses what to do when MI is rejected. In this case, it can be concluded that one or more assessment items exhibit DIF, which is also referred to as partial MI. Methods for inferring which items exhibit DIF were originally developed in the context of item response theory (IRT) rather than factor analysis. This part of the workshop begins with a review of IRT, emphasizing its relationship with factor analysis for categorical data, and then discusses some traditional techniques for DIF analysis (the Mantel-Haenszel test and the likelihood ratio test). Emphasis is given to the problem of anchor item selection.\nPart 3 reviews of some more recent approaches to DIF analysis, focusing on the author’s work on how DIF can be approached as a problem in robust scaling (Halpin, 2022) and extensions of this approach to evaluate differential test functioning (DTF)."
  },
  {
    "objectID": "files/index-eng.html#requirements",
    "href": "files/index-eng.html#requirements",
    "title": "MI / DIF Workshop",
    "section": "Requirements",
    "text": "Requirements\nThe course assumes a background in factor analysis and / or item response theory. Numerical illustrations use the R programming language. To ensure participants are able to follow along with the numerical examples during the workshop, it is recommended to install the following libraries prior to attending.\n\n# Factor analysis\ninstall.packages(\"lavaan\")\n\n# IRT\ninstall.packages(\"mirt\")\ninstall.packages(\"difR\")\n\n# Robust DIF\ninstall.packages(\"remotes\")\nremotes::install_github(\"peterhalpin/robustDIF\")\n\n# Plotting\ninstall.packages(\"ggplot2\")"
  },
  {
    "objectID": "files/index-eng.html#data",
    "href": "files/index-eng.html#data",
    "title": "MI / DIF Workshop",
    "section": "Data",
    "text": "Data\nThe data set linked below is used for numerical illustrations. Note that access to the data is password protected. The password will be shared with participants in the first session of the workshop\n\nLink to data"
  },
  {
    "objectID": "files/index-eng.html#schedule-and-materials",
    "href": "files/index-eng.html#schedule-and-materials",
    "title": "MI / DIF Workshop",
    "section": "Schedule and Materials",
    "text": "Schedule and Materials\n\n\n\nSession\nMaterials\nSupplementary\n\n\n\n\nPart 1\nSlides\nNotes\nCode\n\n\nReview of MI and DIF: Thissen, 2023\nMI for categorical data: Muthen & Asparouhov, 2002\nMillsap & Yun-Tein, 2004; Wu & Estabrook, 2016\n\n\n\nPart 2\nSlides\nNotes\nCode\n\n\nFactor analysis and IRT: Wirth & Edwards, 2007\nReview of DIF: Teresi et. al, 2021\n\n\n\nPart 3\nSlides\nNotes\nCode\n\n\nRobust scaling: He & Cui, 2020\nRobust DIF: Halpin, 2022; Wang et al.,2022"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "",
    "text": "El énfasis de este taller está en evaluar la estructura y comparar pruebas educativas y psicológicas en grupos independientes de encuestados. A menudo, queremos utilizar datos de pruebas para comparar grupos usando el constructo que se está midiendo, por ejemplo, al comparar las medias del grupo en la puntuación total de la prueba. Sin embargo, las puntuaciones de las pruebas también pueden estar influenciadas por atributos del encuestado que no sean el rasgo que se está midiendo. En los Estados Unidos, un ejemplo común son las evaluaciones de la depresión que preguntan si una persona ha llorado en la última semana. Los hombres tienen menos probabilidad de respaldar este ítem que las mujeres, incluso cuando tienen el mismo nivel de depresión (Bauer, 2017). Por lo tanto, comparar hombres y mujeres en términos de su puntuación total en la prueba llevaría a subestimar el nivel de depresión en los hombres.\nEste taller introduce nuevas herramientas de código abierto que prometen facilitar el análisis y la desagregación de datos de pruebas educativas y psicológicas, garantizando la equidad de los datos. Cubriendo el análisis factorial k-fold (kfa), la invarianza de medición (IM) y el funcionamiento diferencial de ítems (FDI), el taller se organiza de la siguiente manera:\n\nLa parte 1 aborda cómo establecer la estructura de la evaluación: ¿Cuántas dimensiones tiene la evaluación? ¿Qué ítems corresponden a qué dimensiones? En este entrenamiento, presentaremos un nuevo recurso de código abierto para identificar y confirmar eficientemente la estructura de una evaluación: el paquete R de validación cruzada k-fold para análisis factorial (kfa). Kfa combina un enfoque analítico factorial basado en modelos exploratorios y confirmatorios que aprovecha eficientemente todos los datos disponibles y proporciona información sintetizada fácil de usar sobre la estructura de una evaluación, ayudando a lograr un consenso en torno al desarrollo y puntuación de escalas.\nLa parte 2 aborda la Invarianza de Medición (MI) en el modelo de factor unidimensional para datos categóricos. El enfoque se centra en varios “niveles” de MI que se obtienen al imponer restricciones cada vez más estrictas en el modelo (por ejemplo, configuracional, débil/métrica, fuerte/escalar) y cómo estas restricciones pueden ser probadas utilizando datos empíricos.\nLa parte 3 aborda qué hacer cuando se rechaza la Invarianza de Medición (MI, por sus siglas en inglés). En este caso, se puede concluir que uno o más elementos de evaluación muestran DIF, también conocido como MI parcial. Los métodos para inferir qué elementos muestran DIF fueron desarrollados originalmente en el contexto de la teoría de respuesta al ítem (IRT, por sus siglas en inglés), en lugar del análisis factorial. Esta parte del taller comienza con una revisión de la IRT, enfatizando su relación con el análisis factorial para datos categóricos, y luego se discuten algunas técnicas tradicionales de análisis de DIF (la prueba de Mantel-Haenszel y la de razón de verosimilitud). Se presta especial atención al problema de la selección de ítems de anclaje.\nLa parte 4 incluye una revisión de algunos enfoques más recientes para el análisis de DIF, centrándose en el trabajo del autor sobre cómo abordar el DIF como un problema para hacer un escalamiento robusto (Halpin, 2022) y extensiones de este enfoque para evaluar si el DIF afecta las conclusiones sobre cómo difieren los grupos en el constructo medido (DTF)."
  },
  {
    "objectID": "index.html#descripción-general",
    "href": "index.html#descripción-general",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "",
    "text": "El énfasis de este taller está en evaluar la estructura y comparar pruebas educativas y psicológicas en grupos independientes de encuestados. A menudo, queremos utilizar datos de pruebas para comparar grupos usando el constructo que se está midiendo, por ejemplo, al comparar las medias del grupo en la puntuación total de la prueba. Sin embargo, las puntuaciones de las pruebas también pueden estar influenciadas por atributos del encuestado que no sean el rasgo que se está midiendo. En los Estados Unidos, un ejemplo común son las evaluaciones de la depresión que preguntan si una persona ha llorado en la última semana. Los hombres tienen menos probabilidad de respaldar este ítem que las mujeres, incluso cuando tienen el mismo nivel de depresión (Bauer, 2017). Por lo tanto, comparar hombres y mujeres en términos de su puntuación total en la prueba llevaría a subestimar el nivel de depresión en los hombres.\nEste taller introduce nuevas herramientas de código abierto que prometen facilitar el análisis y la desagregación de datos de pruebas educativas y psicológicas, garantizando la equidad de los datos. Cubriendo el análisis factorial k-fold (kfa), la invarianza de medición (IM) y el funcionamiento diferencial de ítems (FDI), el taller se organiza de la siguiente manera:\n\nLa parte 1 aborda cómo establecer la estructura de la evaluación: ¿Cuántas dimensiones tiene la evaluación? ¿Qué ítems corresponden a qué dimensiones? En este entrenamiento, presentaremos un nuevo recurso de código abierto para identificar y confirmar eficientemente la estructura de una evaluación: el paquete R de validación cruzada k-fold para análisis factorial (kfa). Kfa combina un enfoque analítico factorial basado en modelos exploratorios y confirmatorios que aprovecha eficientemente todos los datos disponibles y proporciona información sintetizada fácil de usar sobre la estructura de una evaluación, ayudando a lograr un consenso en torno al desarrollo y puntuación de escalas.\nLa parte 2 aborda la Invarianza de Medición (MI) en el modelo de factor unidimensional para datos categóricos. El enfoque se centra en varios “niveles” de MI que se obtienen al imponer restricciones cada vez más estrictas en el modelo (por ejemplo, configuracional, débil/métrica, fuerte/escalar) y cómo estas restricciones pueden ser probadas utilizando datos empíricos.\nLa parte 3 aborda qué hacer cuando se rechaza la Invarianza de Medición (MI, por sus siglas en inglés). En este caso, se puede concluir que uno o más elementos de evaluación muestran DIF, también conocido como MI parcial. Los métodos para inferir qué elementos muestran DIF fueron desarrollados originalmente en el contexto de la teoría de respuesta al ítem (IRT, por sus siglas en inglés), en lugar del análisis factorial. Esta parte del taller comienza con una revisión de la IRT, enfatizando su relación con el análisis factorial para datos categóricos, y luego se discuten algunas técnicas tradicionales de análisis de DIF (la prueba de Mantel-Haenszel y la de razón de verosimilitud). Se presta especial atención al problema de la selección de ítems de anclaje.\nLa parte 4 incluye una revisión de algunos enfoques más recientes para el análisis de DIF, centrándose en el trabajo del autor sobre cómo abordar el DIF como un problema para hacer un escalamiento robusto (Halpin, 2022) y extensiones de este enfoque para evaluar si el DIF afecta las conclusiones sobre cómo difieren los grupos en el constructo medido (DTF)."
  },
  {
    "objectID": "index.html#requisitos",
    "href": "index.html#requisitos",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "Requisitos",
    "text": "Requisitos\nEl curso presupone conocimientos sobre análisis factorial y/o teoría de respuesta al ítem. Las ilustraciones numéricas utilizan el lenguaje de programación R. Para asegurarse de poder seguir los ejemplos numéricos, se recomienda instalar las siguientes bibliotecas antes de asistir al taller.\n\n# Data manipulation and summary statistics\ninstall.packages(\"psych\")\ninstall.packages(\"gtsummary\")\ninstall.packages(\"dplyr\")\n\n# Factor analysis\ninstall.packages (\"kfa\")\ninstall.packages(\"lavaan\")\n\n# IRT\ninstall.packages(\"mirt\")\ninstall.packages(\"difR\")\n\n# Robust DIF\ninstall.packages(\"remotes\")\nremotes::install_github(\"peterhalpin/robustDIF\")\n\n# Plotting\ninstall.packages(\"ggplot2\")"
  },
  {
    "objectID": "index.html#datos",
    "href": "index.html#datos",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "Datos",
    "text": "Datos\nEl conjunto de datos a continuación se utiliza para hacer ilustraciones numéricas. Tenga en cuenta que el acceso a los datos está protegido por una contraseña. La contraseña se compartirá con los participantes en la primera sesión del taller.\n\nLink to data"
  },
  {
    "objectID": "index.html#agenda-y-materiales",
    "href": "index.html#agenda-y-materiales",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "Agenda y materiales",
    "text": "Agenda y materiales\n\n\n\nSesion\nMateriales\nSuplementos\n\n\n\n\nPre-workshop R prep\nCode\n\n\n\nDay 1: KFA\nSlides\nCode: Descriptive\nCode: kfa\n\n\n\n\nDay 2, Part 1: MI\nSlides\nNotes\nCode\n\n\nReview of MI and DIF: Thissen, 2023\nMI for categorical data: Muthen & Asparouhov, 2002\nMillsap & Yun-Tein, 2004; Wu & Estabrook, 2016\n\n\n\nDay 2, Part 2: DIF\nSlides\nNotes\nCode\n\n\nFactor analysis and IRT: Wirth & Edwards, 2007\nReview of DIF: Teresi et. al, 2021\n\n\n\nDay 3, Part 3: RDIF\nSlides\nNotes\nCode\n\n\nRobust scaling: He & Cui, 2020\nRobust DIF: Halpin, 2022; Wang et al.,2022"
  },
  {
    "objectID": "files/index-spa-old.html",
    "href": "files/index-spa-old.html",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "",
    "text": "El énfasis de este taller está en evaluar la estructura y comparar pruebas educativas y psicológicas en grupos independientes de encuestados. A menudo, queremos utilizar datos de pruebas para comparar grupos usando el constructo que se está midiendo, por ejemplo, al comparar las medias del grupo en la puntuación total de la prueba. Sin embargo, las puntuaciones de las pruebas también pueden estar influenciadas por atributos del encuestado que no sean el rasgo que se está midiendo. En los Estados Unidos, un ejemplo común son las evaluaciones de la depresión que preguntan si una persona ha llorado en la última semana. Los hombres tienen menos probabilidad de respaldar este ítem que las mujeres, incluso cuando tienen el mismo nivel de depresión (Bauer, 2017). Por lo tanto, comparar hombres y mujeres en términos de su puntuación total en la prueba llevaría a subestimar el nivel de depresión en los hombres.\nEste taller introduce nuevas herramientas de código abierto que prometen facilitar el análisis y la desagregación de datos de pruebas educativas y psicológicas, garantizando la equidad de los datos. Cubriendo el análisis factorial k-fold (kfa), la invarianza de medición (IM) y el funcionamiento diferencial de ítems (FDI), el taller se organiza de la siguiente manera:\n\nLa parte 1 aborda cómo establecer la estructura de la evaluación: ¿Cuántas dimensiones tiene la evaluación? ¿Qué ítems corresponden a qué dimensiones? En este entrenamiento, presentaremos un nuevo recurso de código abierto para identificar y confirmar eficientemente la estructura de una evaluación: el paquete R de validación cruzada k-fold para análisis factorial (kfa). Kfa combina un enfoque analítico factorial basado en modelos exploratorios y confirmatorios que aprovecha eficientemente todos los datos disponibles y proporciona información sintetizada fácil de usar sobre la estructura de una evaluación, ayudando a lograr un consenso en torno al desarrollo y puntuación de escalas.\nLa parte 2 aborda la Invarianza de Medición (MI) en el modelo de factor unidimensional para datos categóricos. El enfoque se centra en varios “niveles” de MI que se obtienen al imponer restricciones cada vez más estrictas en el modelo (por ejemplo, configuracional, débil/métrica, fuerte/escalar) y cómo estas restricciones pueden ser probadas utilizando datos empíricos.\nLa parte 3 aborda qué hacer cuando se rechaza la Invarianza de Medición (MI, por sus siglas en inglés). En este caso, se puede concluir que uno o más elementos de evaluación muestran DIF, también conocido como MI parcial. Los métodos para inferir qué elementos muestran DIF fueron desarrollados originalmente en el contexto de la teoría de respuesta al ítem (IRT, por sus siglas en inglés), en lugar del análisis factorial. Esta parte del taller comienza con una revisión de la IRT, enfatizando su relación con el análisis factorial para datos categóricos, y luego se discuten algunas técnicas tradicionales de análisis de DIF (la prueba de Mantel-Haenszel y la de razón de verosimilitud). Se presta especial atención al problema de la selección de ítems de anclaje.\nLa parte 4 incluye una revisión de algunos enfoques más recientes para el análisis de DIF, centrándose en el trabajo del autor sobre cómo abordar el DIF como un problema para hacer un escalamiento robusto (Halpin, 2022) y extensiones de este enfoque para evaluar si el DIF afecta las conclusiones sobre cómo difieren los grupos en el constructo medido (DTF)."
  },
  {
    "objectID": "files/index-spa-old.html#descripción-general",
    "href": "files/index-spa-old.html#descripción-general",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "",
    "text": "El énfasis de este taller está en evaluar la estructura y comparar pruebas educativas y psicológicas en grupos independientes de encuestados. A menudo, queremos utilizar datos de pruebas para comparar grupos usando el constructo que se está midiendo, por ejemplo, al comparar las medias del grupo en la puntuación total de la prueba. Sin embargo, las puntuaciones de las pruebas también pueden estar influenciadas por atributos del encuestado que no sean el rasgo que se está midiendo. En los Estados Unidos, un ejemplo común son las evaluaciones de la depresión que preguntan si una persona ha llorado en la última semana. Los hombres tienen menos probabilidad de respaldar este ítem que las mujeres, incluso cuando tienen el mismo nivel de depresión (Bauer, 2017). Por lo tanto, comparar hombres y mujeres en términos de su puntuación total en la prueba llevaría a subestimar el nivel de depresión en los hombres.\nEste taller introduce nuevas herramientas de código abierto que prometen facilitar el análisis y la desagregación de datos de pruebas educativas y psicológicas, garantizando la equidad de los datos. Cubriendo el análisis factorial k-fold (kfa), la invarianza de medición (IM) y el funcionamiento diferencial de ítems (FDI), el taller se organiza de la siguiente manera:\n\nLa parte 1 aborda cómo establecer la estructura de la evaluación: ¿Cuántas dimensiones tiene la evaluación? ¿Qué ítems corresponden a qué dimensiones? En este entrenamiento, presentaremos un nuevo recurso de código abierto para identificar y confirmar eficientemente la estructura de una evaluación: el paquete R de validación cruzada k-fold para análisis factorial (kfa). Kfa combina un enfoque analítico factorial basado en modelos exploratorios y confirmatorios que aprovecha eficientemente todos los datos disponibles y proporciona información sintetizada fácil de usar sobre la estructura de una evaluación, ayudando a lograr un consenso en torno al desarrollo y puntuación de escalas.\nLa parte 2 aborda la Invarianza de Medición (MI) en el modelo de factor unidimensional para datos categóricos. El enfoque se centra en varios “niveles” de MI que se obtienen al imponer restricciones cada vez más estrictas en el modelo (por ejemplo, configuracional, débil/métrica, fuerte/escalar) y cómo estas restricciones pueden ser probadas utilizando datos empíricos.\nLa parte 3 aborda qué hacer cuando se rechaza la Invarianza de Medición (MI, por sus siglas en inglés). En este caso, se puede concluir que uno o más elementos de evaluación muestran DIF, también conocido como MI parcial. Los métodos para inferir qué elementos muestran DIF fueron desarrollados originalmente en el contexto de la teoría de respuesta al ítem (IRT, por sus siglas en inglés), en lugar del análisis factorial. Esta parte del taller comienza con una revisión de la IRT, enfatizando su relación con el análisis factorial para datos categóricos, y luego se discuten algunas técnicas tradicionales de análisis de DIF (la prueba de Mantel-Haenszel y la de razón de verosimilitud). Se presta especial atención al problema de la selección de ítems de anclaje.\nLa parte 4 incluye una revisión de algunos enfoques más recientes para el análisis de DIF, centrándose en el trabajo del autor sobre cómo abordar el DIF como un problema para hacer un escalamiento robusto (Halpin, 2022) y extensiones de este enfoque para evaluar si el DIF afecta las conclusiones sobre cómo difieren los grupos en el constructo medido (DTF)."
  },
  {
    "objectID": "files/index-spa-old.html#requisitos",
    "href": "files/index-spa-old.html#requisitos",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "Requisitos",
    "text": "Requisitos\nEl curso presupone conocimientos sobre análisis factorial y/o teoría de respuesta al ítem. Las ilustraciones numéricas utilizan el lenguaje de programación R. Para asegurarse de poder seguir los ejemplos numéricos, se recomienda instalar las siguientes bibliotecas antes de asistir al taller.\n\n# Data manipulation and summary statistics\ninstall.packages(\"psych\")\ninstall.packages(\"gtsummary\")\ninstall.packages(\"dplyr\")\n\n# Factor analysis\ninstall.packages (\"kfa\")\ninstall.packages(\"lavaan\")\n\n# IRT\ninstall.packages(\"mirt\")\ninstall.packages(\"difR\")\n\n# Robust DIF\ninstall.packages(\"remotes\")\nremotes::install_github(\"peterhalpin/robustDIF\")\n\n# Plotting\ninstall.packages(\"ggplot2\")"
  },
  {
    "objectID": "files/index-spa-old.html#datos",
    "href": "files/index-spa-old.html#datos",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "Datos",
    "text": "Datos\nEl conjunto de datos a continuación se utiliza para hacer ilustraciones numéricas. Tenga en cuenta que el acceso a los datos está protegido por una contraseña. La contraseña se compartirá con los participantes en la primera sesión del taller.\n\nLink to data"
  },
  {
    "objectID": "files/index-spa-old.html#agenda-y-materiales",
    "href": "files/index-spa-old.html#agenda-y-materiales",
    "title": "Desde los Datos a la Desagregación hasta las Decisiones",
    "section": "Agenda y materiales",
    "text": "Agenda y materiales\n\n\n\nSesion\nMateriales\nSuplementos |\n\n\n\n\nPart 1\n[Slides] [Slides Esp] [Code]\n\n\n\nPart 2\nSlides\nNotes\nCode\n\n\nReview of MI and DIF: Thissen, 2023\nMI for categorical data: Muthen & Asparouhov, 2002\nMillsap & Yun-Tein, 2004; Wu & Estabrook, 2016\n\n\n\nPart 3\nSlides\nNotes\nCode\n\n\nFactor analysis and IRT: Wirth & Edwards, 2007\nReview of DIF: Teresi et. al, 2021\n\n\n\nPart 4\nSlides\nNotes\nCode\n\n\nRobust scaling: He & Cui, 2020\nRobust DIF: Halpin, 2022; Wang et al.,2022"
  }
]